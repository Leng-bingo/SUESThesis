# ResNet（2015）

提出问题：**随着网络越来越深，梯度就会出现爆炸或者消失**


- 文章提出出现精度变差的问题不是因为层数变多了，模型变复杂了导致的过拟合，而是因为训练误差也变高了（overfitting是说训练误差变得很低，但是测试误差变得很高），训练误差和测试误差都变高了，所以他不是overfitting。虽然网络是收敛的，但是好像没有训练出一个好的结果
- 要学的东西叫做**H（x）**，假设现在已经有了一个浅的神经网络，他的输出是x，然后要在这个浅的神经网络上面再新加一些层，让它变得更深。
- 新加的那些层不要直接去学H（x），而是**应该去学H（x）-x**，x是原始的浅层神经网络已经学到的一些东西，新加的层不要重新去学习，而是去学习学到的东西和真实的东西之间的残差，最后整个神经网络的输出等价于浅层神经网络的输出x和新加的神经网络学习残差的输出之和，将优化目标从H（x）转变成为了H（x）-x

---

- 深的神经网络非常难以训练，残差学习，使训练深的网络更容易![](https://leng-mypic.oss-cn-beijing.aliyuncs.com/mac-img/20220513172900.png)

- 上一层的输出加上残差作为下一层的输入
  - 不增加模型复杂度
  - 只是个加法